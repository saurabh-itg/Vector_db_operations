{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c905e621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a586cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17cecd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000026524C05550>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000026524B22B40>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e86011f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Sonu, it's nice to meet you!\\n\\nThat's awesome that you're an AI developer. It's such an exciting field right now. \\n\\nWhat kind of AI projects are you working on? ü§î  I'm always curious to hear about what people are building.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 21, 'total_tokens': 86, 'completion_time': 0.118181818, 'prompt_time': 0.00132736, 'queue_time': 0.193439387, 'total_time': 0.119509178}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--c13fee37-0fd3-4bff-9aef-4191a9d0641e-0', usage_metadata={'input_tokens': 21, 'output_tokens': 65, 'total_tokens': 86})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi , My name is sonu i am an AI developer\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f208d748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You said you're an AI Engineer!  \\n\\nI was just getting ready to ask you more about it.  Tell me, what kind of projects are you working on? What aspects of AI engineering are you most passionate about? üòÑ  \\n\\n\\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 92, 'total_tokens': 145, 'completion_time': 0.096363636, 'prompt_time': 0.002589021, 'queue_time': 0.193666656, 'total_time': 0.098952657}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--8c929c2d-56ab-47cc-a175-916a00d57be5-0', usage_metadata={'input_tokens': 92, 'output_tokens': 53, 'total_tokens': 145})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi , My name is SJ i am an AI Engineer\"),\n",
    "        AIMessage(content=\"Hi SJ, it's nice to meet you! \\n\\nThat's awesome that you're an AI Engineer. What kind of work do you do?  I'm always interested in learning more about how people are using AI to solve problems and create new things.\"),\n",
    "        HumanMessage(content=\"Hey What do I do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d940d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Message History\n",
    "#Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70da63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81e66421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='France won the World Cup in 2018! üá´üá∑  They beat Croatia in the final.  \\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 157, 'total_tokens': 184, 'completion_time': 0.049090909, 'prompt_time': 0.004612289, 'queue_time': 0.190967514, 'total_time': 0.053703198}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--af167c5a-dc08-4f0c-9ee9-3ca072ff06d5-0' usage_metadata={'input_tokens': 157, 'output_tokens': 27, 'total_tokens': 184}\n",
      "content=\"That's a good question!  \\n\\nWhile France won 4-2,  there were actually two players who scored the most goals in the final.\\n\\n* **Kylian Mbapp√©** of France scored **two goals**.\\n* **Mand≈æukiƒá** of Croatia scored **one goal**.  \\n\\n\\n\\nLet me know if you have any other World Cup questions! ‚öΩÔ∏èüèÜ \\n\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 201, 'total_tokens': 285, 'completion_time': 0.152727273, 'prompt_time': 0.005554049, 'queue_time': 0.191093257, 'total_time': 0.158281322}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--cc478369-7b08-4da2-b384-c67c3dc1b8ab-0' usage_metadata={'input_tokens': 201, 'output_tokens': 84, 'total_tokens': 285}\n"
     ]
    }
   ],
   "source": [
    "response1 = with_message_history.invoke(\n",
    "    {\"input\": \"Hi, who won the World Cup in 2018?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user12\"}}\n",
    ")\n",
    "print(response1)\n",
    "\n",
    "response2 = with_message_history.invoke(\n",
    "    {\"input\": \"And who scored most goals in the final?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user12\"}}\n",
    ")\n",
    "print(response2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d4cf3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='France won the World Cup in 2018!  üá´üá∑  üéâ  \\n\\nThey beat Croatia in the final. \\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 307, 'total_tokens': 337, 'completion_time': 0.054545455, 'prompt_time': 0.006666508, 'queue_time': 0.1903082, 'total_time': 0.061211963}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--e63d1a91-6342-4a9f-9bdd-a5ddcefdc57b-0' usage_metadata={'input_tokens': 307, 'output_tokens': 30, 'total_tokens': 337}\n",
      "content='Please provide me with information about which final you are referring to. \\n\\nFor example, tell me:\\n\\n* **The sport:** (e.g., football, basketball, hockey)\\n* **The tournament:** (e.g., World Cup, Champions League, NBA Finals)\\n* **The year:** (e.g., 2022, 2010) \\n\\n\\nOnce you give me these details, I can tell you who scored the most goals in the final. üòä \\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 18, 'total_tokens': 126, 'completion_time': 0.196363636, 'prompt_time': 0.00124702, 'queue_time': 0.18966199, 'total_time': 0.197610656}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--474cb494-5772-433c-8847-3d49630afd95-0' usage_metadata={'input_tokens': 18, 'output_tokens': 108, 'total_tokens': 126}\n"
     ]
    }
   ],
   "source": [
    "response1 = with_message_history.invoke(\n",
    "    {\"input\": \"Hi, who won the World Cup in 2018?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user12\"}}\n",
    ")\n",
    "print(response1)\n",
    "\n",
    "response2 = with_message_history.invoke(\n",
    "    {\"input\": \"And who scored most goals in the final?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user1\"}}\n",
    ")\n",
    "print(response2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc9d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prompt templates\n",
    "#Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8030bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MessagesPlaceholder(variable_name=\"messages\") is a slot in the template where you can insert a list of past chat messages dynamically.\n",
    "#MessagesPlaceholder is like a dynamic insertion point in your prompt template where past conversation turns (or any list of messages) can be injected at runtime. It‚Äôs essential if you want your assistant to maintain context across multiple interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6278ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Answer all the question to the best of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e18fe0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"It's great that you're thinking about your health! \\n\\nWhile your blood type is A+, it doesn't directly dictate how you should take care of your health.  \\n\\n**Here's what's important for everyone, regardless of blood type:**\\n\\n* **Balanced Diet:** \\n    * Focus on fruits, vegetables, whole grains, lean proteins, and healthy fats.\\n    * Limit processed foods, sugary drinks, and unhealthy fats.\\n* **Regular Exercise:**\\n    * Aim for at least 150 minutes of moderate-intensity aerobic activity or 75 minutes of vigorous-intensity aerobic activity per week.\\n    * Include strength training exercises at least twice a week.\\n* **Adequate Sleep:**\\n    * Get 7-9 hours of quality sleep each night.\\n* **Stress Management:**\\n    * Find healthy ways to manage stress, such as exercise, yoga, meditation, or spending time in nature.\\n* **Regular Medical Checkups:**\\n    * See your doctor for regular checkups and screenings. This helps detect potential health problems early on.\\n* **Hydration:**\\n    * Drink plenty of water throughout the day.\\n\\n**Important Note:**\\n\\nThere's no scientific evidence to support the idea that specific diets or lifestyle choices are better for people based on their blood type.  While some online resources might promote these ideas, they are not supported by medical professionals.\\n\\n\\n**Talk to Your Doctor:**\\n\\nThe best way to get personalized health advice is to talk to your doctor. They can assess your individual needs and recommend the best course of action for you.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 337, 'prompt_tokens': 39, 'total_tokens': 376, 'completion_time': 0.612727273, 'prompt_time': 0.001568727, 'queue_time': 0.191869395, 'total_time': 0.614296}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--d4d97dc8-5340-421b-81ce-82a61081b75a-0', usage_metadata={'input_tokens': 39, 'output_tokens': 337, 'total_tokens': 376})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi My blood is A+ how should i take care of my health\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20ee3bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2d71264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"It's great that you're thinking about your health!  Knowing your blood type is interesting, but it doesn't directly tell us how to specifically care for your health. \\n\\n**Here are some general tips for healthy living that apply to everyone, regardless of blood type:**\\n\\n* **Eat a balanced diet:** Focus on fruits, vegetables, whole grains, lean protein, and healthy fats.\\n\\n* **Stay hydrated:** Drink plenty of water throughout the day.\\n* **Get regular exercise:** Aim for at least 30 minutes of moderate-intensity exercise most days of the week.\\n* **Get enough sleep:** Most adults need 7-9 hours of sleep per night.\\n* **Manage stress:** Find healthy ways to cope with stress, such as exercise, yoga, or meditation.\\n* **Don't smoke:** Smoking is incredibly harmful to your health.\\n* **Limit alcohol consumption:** If you drink alcohol, do so in moderation.\\n* **Get regular medical checkups:** See your doctor for regular checkups and screenings.\\n\\n**It's important to note that there is no scientific evidence to support the idea that blood type affects diet or health recommendations.**\\n\\n\\n**However, if you have any specific health concerns or questions, it's always best to talk to your doctor.** They can provide personalized advice based on your individual needs and medical history.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 286, 'prompt_tokens': 39, 'total_tokens': 325, 'completion_time': 0.52, 'prompt_time': 0.001565648, 'queue_time': 0.193063554, 'total_time': 0.521565648}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--d723e03c-91f2-4da9-aa31-b3f1d3b51998-0', usage_metadata={'input_tokens': 39, 'output_tokens': 286, 'total_tokens': 325})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My blood is A+ how should i take care of my health\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e74b1ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You told me your blood type is A+.  \\n\\nIs there anything else I can help you with?\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my blood group?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60b08be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add more complexity\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e5b248c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á SJ!\\n\\n‡§ï‡§ø‡§∏‡•ã‡§∞ ‡§ï‡•Å‡§Æ‡§æ‡§∞ ‡§®‡•á ‡§¨‡§π‡•Å‡§§ ‡§∏‡§æ‡§∞‡•á ‡§¶‡•á‡§∂‡§≠‡§ï‡•ç‡§§ ‡§ó‡•Ä‡§§ ‡§ó‡§æ‡§è ‡§π‡•à‡§Ç!  \"‡§ê ‡§Æ‡•á‡§∞‡•á ‡§µ‡§§‡§® ‡§ï‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç\" ‡§è‡§ï ‡§¨‡§π‡•Å‡§§ ‡§π‡•Ä ‡§™‡•ç‡§∞‡§∏‡§ø‡§¶‡•ç‡§ß ‡§î‡§∞ ‡§™‡•ç‡§∞‡•á‡§∞‡§£‡§æ‡§¶‡§æ‡§Ø‡§ï ‡§¶‡•á‡§∂‡§≠‡§ï‡•ç‡§§ ‡§ó‡•Ä‡§§ ‡§π‡•à ‡§ú‡§ø‡§∏‡•á ‡§â‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§ó‡§æ‡§Ø‡§æ ‡§π‡•à‡•§ \\n\\n‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§ö‡§æ‡§π‡•á‡§Ç‡§ó‡•á ‡§ï‡§ø ‡§Æ‡•à‡§Ç ‡§ï‡•Å‡§õ ‡§î‡§∞ ‡§ó‡§æ‡§®‡•á ‡§ï‡•á ‡§®‡§æ‡§Æ ‡§∏‡•Å‡§ù‡§æ‡§ä‡§Å? \\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is SJ ,can you tell a bollywood patriotic song by kishor da\")],\"language\":\"hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa18ca99",
   "metadata": {},
   "source": [
    "#Let's now wrap this more complicated chain in a Message History class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7eab26db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db59bd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á SJ,  ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡§π‡§æ‡§Å ‡§π‡•Ç‡§Å‡•§ ‡§Ü‡§™‡§ï‡•á ‡§ï‡•ã‡§à ‡§∏‡§µ‡§æ‡§≤ ‡§π‡•à‡§Ç? üòä \\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "repsonse=with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"Hi,I am SJ\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "repsonse.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64d385c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \"language\": \"Hindi\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5019038",
   "metadata": {},
   "source": [
    "### Managing the Conversation History-- giving only required context to llm \n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "'trim_messages' helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49bcf4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b163b8ef6574d7f94ecd28ce2cccd46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OFA2KOR\\Desktop\\vector_db\\env_genai\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\OFA2KOR\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f3700b459147aeba3e51a52724e08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9a280d69594ff2a8f50fe86284cef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be80b9734dd40c48156c890fe6042fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa725ea04b1d49448b75d98145300a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4e56345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a helpful assistant, I don't have personal memories or preferences like liking a particular ice cream flavor. \\n\\nTo help you decide, could you tell me what flavors you usually enjoy?  \\n\\nFor example, do you like:\\n\\n* **Chocolatey flavors?**\\n* **Fruity flavors?**\\n* **Something creamy and vanilla-based?**\\n* **Unique or adventurous flavors?**\\n\\n\\n\\nLet me know, and I can give you some delicious suggestions! üç¶ üòä\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    "    \n",
    ")\n",
    "\n",
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What icecream i liked\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfa4e740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked \"what\\'s 2 + 2\". üòä  \\n\\n\\n\\nIs there anything else I can help you with?\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c000e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets wrap this in the MEssage History\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33abb09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have access to past conversations or any information about you personally, including your name.  \\n\\nWould you like to tell me your name? üòä\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c233c54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a helpful assistant, I have no memory of past conversations.  \\n\\nIf you'd like to ask me a math problem, I'm happy to help! üòä  Just let me know what it is. \\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what math problem did i ask?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9cf19df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector stores and retrievers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1e6f89",
   "metadata": {},
   "source": [
    "These abstraction are designed to support retrieval of data from vector databases and other sources for integration with LLM workflows. They are important for application that fetch data to be reasoned over as part of model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a26bde6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents\n",
    "#vector stores\n",
    "#retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1fd364",
   "metadata": {},
   "source": [
    "#documents-in LangChain, a Document is just a little wrapper around: page_content: the actual text. metadata: any structured info about that text (source, chunk id, author, etc.).\n",
    "The metadata dict is completely flexible ‚Äî you can store anything useful for retrieval/filtering (like tags, categories, IDs, sources).\n",
    "\n",
    "In practice, when you split PDFs, scrape web pages, or load CSVs, each chunk gets wrapped into one of these Document objects before being embedded and stored in a vector DB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9dcb7891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Hello, world!\",\n",
    "        metadata={\"source\": \"https://example.com\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LangChain makes it easy to build LLM-powered applications.\",\n",
    "        metadata={\"source\": \"docs/langchain\", \"category\": \"AI frameworks\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Pandas is a Python library for data analysis and manipulation.\",\n",
    "        metadata={\"source\": \"wikipedia\", \"topic\": \"data science\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"The Eiffel Tower is located in Paris and is 330 meters tall.\",\n",
    "        metadata={\"source\": \"travel_guide\", \"city\": \"Paris\", \"type\": \"landmark\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"To brew coffee, first grind the beans, then pour hot water over them.\",\n",
    "        metadata={\"source\": \"howto/coffee\", \"difficulty\": \"easy\", \"author\": \"Barista Bob\"}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33c9acf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://example.com'}, page_content='Hello, world!'),\n",
       " Document(metadata={'source': 'docs/langchain', 'category': 'AI frameworks'}, page_content='LangChain makes it easy to build LLM-powered applications.'),\n",
       " Document(metadata={'source': 'wikipedia', 'topic': 'data science'}, page_content='Pandas is a Python library for data analysis and manipulation.'),\n",
       " Document(metadata={'source': 'travel_guide', 'city': 'Paris', 'type': 'landmark'}, page_content='The Eiffel Tower is located in Paris and is 330 meters tall.'),\n",
       " Document(metadata={'source': 'howto/coffee', 'difficulty': 'easy', 'author': 'Barista Bob'}, page_content='To brew coffee, first grind the beans, then pour hot water over them.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f070d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c22a622af7846e1a9e7f6f88e6e6738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OFA2KOR\\Desktop\\vector_db\\env_genai\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\OFA2KOR\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ffaca47a5da476d982b73138ee605c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83dc09859e44ce3bfd798cfc1c19c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464d30ab46294ab68fe6b6cea0566df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f070637a7fe4a209a5c88faa31168db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63258e6b76d4990b068ac341f14e1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf33cee38cc4a98976ccc2c99b33795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3907b4e1bbc6407f9a697a7ef0923eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a47a8faaea40278b0e415860cd927d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfca83126f04ca489b0ea61e3ede1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106ea6b32b8b475aac9e096e2605110a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#vectorstores\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "embeddings=HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3591164",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore =Chroma.from_documents(documents,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1cee093e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x265766e2000>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f595c589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='e3ea7c89-63ee-4959-ad71-2b75d1def8a0', metadata={'source': 'https://example.com'}, page_content='Hello, world!'),\n",
       " Document(id='604c1ad9-9c6a-41f1-8300-6b7f7c6b8cc5', metadata={'category': 'AI frameworks', 'source': 'docs/langchain'}, page_content='LangChain makes it easy to build LLM-powered applications.'),\n",
       " Document(id='cdf5ac4d-4ae3-49d0-b4cb-9d9843faacee', metadata={'source': 'howto/coffee', 'author': 'Barista Bob', 'difficulty': 'easy'}, page_content='To brew coffee, first grind the beans, then pour hot water over them.'),\n",
       " Document(id='29ad7dfa-7dd7-4897-bff4-770ee146d566', metadata={'topic': 'data science', 'source': 'wikipedia'}, page_content='Pandas is a Python library for data analysis and manipulation.')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search('ChatGroq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59cac491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='e3ea7c89-63ee-4959-ad71-2b75d1def8a0', metadata={'source': 'https://example.com'}, page_content='Hello, world!'),\n",
       " Document(id='cdf5ac4d-4ae3-49d0-b4cb-9d9843faacee', metadata={'author': 'Barista Bob', 'source': 'howto/coffee', 'difficulty': 'easy'}, page_content='To brew coffee, first grind the beans, then pour hot water over them.'),\n",
       " Document(id='29ad7dfa-7dd7-4897-bff4-770ee146d566', metadata={'topic': 'data science', 'source': 'wikipedia'}, page_content='Pandas is a Python library for data analysis and manipulation.'),\n",
       " Document(id='aafc8254-3b89-43d5-aac6-c667c48271fc', metadata={'source': 'travel_guide', 'type': 'landmark', 'city': 'Paris'}, page_content='The Eiffel Tower is located in Paris and is 330 meters tall.')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await vectorstore.asimilarity_search('Cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e11f3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='e3ea7c89-63ee-4959-ad71-2b75d1def8a0', metadata={'source': 'https://example.com'}, page_content='Hello, world!'),\n",
       "  1.6167809963226318),\n",
       " (Document(id='604c1ad9-9c6a-41f1-8300-6b7f7c6b8cc5', metadata={'source': 'docs/langchain', 'category': 'AI frameworks'}, page_content='LangChain makes it easy to build LLM-powered applications.'),\n",
       "  1.6783924102783203),\n",
       " (Document(id='cdf5ac4d-4ae3-49d0-b4cb-9d9843faacee', metadata={'author': 'Barista Bob', 'difficulty': 'easy', 'source': 'howto/coffee'}, page_content='To brew coffee, first grind the beans, then pour hot water over them.'),\n",
       "  1.6892012357711792),\n",
       " (Document(id='29ad7dfa-7dd7-4897-bff4-770ee146d566', metadata={'source': 'wikipedia', 'topic': 'data science'}, page_content='Pandas is a Python library for data analysis and manipulation.'),\n",
       "  1.9206647872924805)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_score('ChatGroq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e68a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff766f57",
   "metadata": {},
   "source": [
    "\n",
    "* **Vector store objects** (e.g., a `FAISS` or `Chroma` instance in LangChain) **do not subclass `Runnable`**.\n",
    "\n",
    "  * That means they don‚Äôt implement the `invoke()`, `ainvoke()`, or `batch()` methods.\n",
    "  * Because of that, you **cannot directly ‚Äúpipe‚Äù them into a LangChain Expression Language (LCEL) chain** (the `|` operator).\n",
    "\n",
    "* **Retrievers** *do* subclass `Runnable`.\n",
    "\n",
    "  * They expose a clean interface:\n",
    "\n",
    "    ```python\n",
    "    retriever.invoke(\"query\")  \n",
    "    ‚Üí returns [Document, Document, ...]\n",
    "    ```\n",
    "  * Because of this, retrievers *can* be plugged directly into LCEL pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Why this matters**\n",
    "\n",
    "* LCEL (`prompt | model | output_parser`) expects each component to be a **Runnable** ‚Äî i.e., it supports:\n",
    "\n",
    "  * `invoke()` (single call),\n",
    "  * `ainvoke()` (async),\n",
    "  * `batch()` (batch processing).\n",
    "\n",
    "* A **vector store** is lower-level:\n",
    "\n",
    "  * It has methods like `.similarity_search(query)` or `.add_texts()`.\n",
    "  * These aren‚Äôt standardized to the LCEL `Runnable` interface.\n",
    "\n",
    "* A **retriever** is a wrapper around a vector store (or another backend) that *does* expose the `Runnable` interface.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Analogy**\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "* **Vector Store** = A raw SQL database connection.\n",
    "  You can run queries, insert rows, but it doesn‚Äôt know how to fit into a pipeline automatically.\n",
    "\n",
    "* **Retriever** = A DAO (Data Access Object) that wraps the DB and provides a clean `get_relevant_documents(query)` method.\n",
    "  This object *does* implement the standardized interface expected by LangChain chains.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Concrete Example**\n",
    "\n",
    "‚ùå **Direct vector store (not runnable):**\n",
    "\n",
    "```python\n",
    "vectorstore = FAISS.from_texts([\"doc1\", \"doc2\"], embedding=embeddings)\n",
    "# This won't work in a chain:\n",
    "chain = prompt | vectorstore | model  # ‚ùå Error\n",
    "```\n",
    "\n",
    "‚úÖ **Retriever (runnable):**\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Now it works in LCEL, because retriever implements Runnable\n",
    "chain = retriever | prompt | model  \n",
    "\n",
    "docs = chain.invoke(\"What is doc1?\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ In short\n",
    "\n",
    "That sentence basically means:\n",
    "\n",
    "* A **vector store** is just storage + search logic ‚Üí not directly usable in a LangChain pipeline.\n",
    "* A **retriever** is a **Runnable wrapper** around a vector store (or other retrieval method) that **can** plug into LCEL chains.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0203ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "63a6f198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='aafc8254-3b89-43d5-aac6-c667c48271fc', metadata={'city': 'Paris', 'source': 'travel_guide', 'type': 'landmark'}, page_content='The Eiffel Tower is located in Paris and is 330 meters tall.')],\n",
       " [Document(id='cdf5ac4d-4ae3-49d0-b4cb-9d9843faacee', metadata={'source': 'howto/coffee', 'author': 'Barista Bob', 'difficulty': 'easy'}, page_content='To brew coffee, first grind the beans, then pour hot water over them.')]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever= RunnableLambda(vectorstore.similarity_search).bind(k=1)\n",
    "retriever.batch(['eifel tower','coffee'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33700456",
   "metadata": {},
   "source": [
    "## üü¢ What is `vectorstore.as_retriever()`?\n",
    "\n",
    "When you call:\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever()\n",
    "```\n",
    "\n",
    "you are **turning a vector store (like FAISS, Chroma, Pinecone, etc.) into a retriever**.\n",
    "\n",
    "---\n",
    "\n",
    "## üü¢ Why do we need this?\n",
    "\n",
    "* A **vector store** is like a database for embeddings (semantic vectors).\n",
    "\n",
    "  * You can add documents, store their embeddings, and run similarity search.\n",
    "  * But it only has methods like `.similarity_search(query)` ‚Äî not the standardized interface LangChain uses in chains.\n",
    "\n",
    "* A **retriever** is a **simple wrapper** that says:\n",
    "\n",
    "  > ‚ÄúJust give me the top-k relevant documents for this query.‚Äù\n",
    "\n",
    "  and exposes it with a standard method:\n",
    "\n",
    "  ```python\n",
    "  retriever.get_relevant_documents(query)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## üü¢ Example\n",
    "\n",
    "### Without retriever:\n",
    "\n",
    "```python\n",
    "# Store docs in FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"LangChain is for LLM apps.\", \"Pandas is for data analysis.\"],\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# You must use vectorstore methods directly\n",
    "results = vectorstore.similarity_search(\"What is LangChain?\", k=1)\n",
    "print(results[0].page_content)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "LangChain is for LLM apps.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### With retriever:\n",
    "\n",
    "```python\n",
    "# Turn vectorstore into retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# Now you can use the standard retriever API\n",
    "docs = retriever.get_relevant_documents(\"What is LangChain?\")\n",
    "print(docs[0].page_content)\n",
    "```\n",
    "\n",
    "‚úÖ Same result, but now it uses the **retriever interface**.\n",
    "\n",
    "---\n",
    "\n",
    "## üü¢ Why this is useful\n",
    "\n",
    "1. **Standardization**\n",
    "   Every retriever (vector-based, keyword-based, hybrid, etc.) has the same method:\n",
    "\n",
    "   ```python\n",
    "   get_relevant_documents(query)\n",
    "   ```\n",
    "\n",
    "2. **Integration with chains**\n",
    "   Retrievers in LangChain are `Runnable`, so you can plug them directly into pipelines:\n",
    "\n",
    "   ```python\n",
    "   chain = retriever | prompt | model\n",
    "   ```\n",
    "\n",
    "3. **Configurable**\n",
    "   You can pass arguments like:\n",
    "\n",
    "   ```python\n",
    "   retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})\n",
    "   ```\n",
    "\n",
    "   * `k` = number of results to return.\n",
    "   * `search_type` = \"similarity\", \"mmr\" (max marginal relevance), etc.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ In short\n",
    "\n",
    "* `vectorstore.as_retriever()` = **wrapper that makes a vector DB act like a retriever**.\n",
    "* It gives you a clean, standard way to fetch documents for a query.\n",
    "* That retriever can be used directly in **RAG pipelines** with LLMs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d67b997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='aafc8254-3b89-43d5-aac6-c667c48271fc', metadata={'type': 'landmark', 'source': 'travel_guide', 'city': 'Paris'}, page_content='The Eiffel Tower is located in Paris and is 330 meters tall.'),\n",
       "  Document(id='604c1ad9-9c6a-41f1-8300-6b7f7c6b8cc5', metadata={'source': 'docs/langchain', 'category': 'AI frameworks'}, page_content='LangChain makes it easy to build LLM-powered applications.')],\n",
       " [Document(id='cdf5ac4d-4ae3-49d0-b4cb-9d9843faacee', metadata={'source': 'howto/coffee', 'author': 'Barista Bob', 'difficulty': 'easy'}, page_content='To brew coffee, first grind the beans, then pour hot water over them.'),\n",
       "  Document(id='e3ea7c89-63ee-4959-ad71-2b75d1def8a0', metadata={'source': 'https://example.com'}, page_content='Hello, world!')]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":2}\n",
    ")\n",
    "retriever.batch(['eifel tower','coffee'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9cfae297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "message = \"\"\"\n",
    "You are a helpful assistant. Use the following pieces of context to answer this question .\n",
    "{question}\n",
    "context:\n",
    "\n",
    "{context}\"\"\"\n",
    "prompt =ChatPromptTemplate.from_messages([('human',message)])\n",
    "rag_chain = {'context': retriever , 'question': RunnablePassthrough()}| prompt | model\n",
    "response=rag_chain.invoke('Where is eifel tower located?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba250925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Eiffel Tower is located in **Paris**. \n",
      "\n",
      "This information can be found in the first document provided. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a72fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
